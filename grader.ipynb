{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run these after removing hastag if any on of the given module is missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hastag if having problem with gensim\n",
    "import sys\n",
    "\n",
    "#!$sys.executable -m pip install keras\n",
    "#!$sys.executable -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "#!$sys.executable -m pip install gensim\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#!$sys.executable -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"Dataset and discription/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(columns=['rater1_domain1','rater2_domain1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score  \n",
       "0                  8  \n",
       "1                  9  \n",
       "2                  7  \n",
       "3                 10  \n",
       "4                  8  \n",
       "...              ...  \n",
       "12971             35  \n",
       "12972             32  \n",
       "12973             40  \n",
       "12974             40  \n",
       "12975             40  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=X['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n",
    "maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score  score  \n",
       "0                  8    6.0  \n",
       "1                  9    7.0  \n",
       "2                  7    5.0  \n",
       "3                 10    8.0  \n",
       "4                  8    6.0  \n",
       "...              ...    ...  \n",
       "12971             35    6.0  \n",
       "12972             32    5.0  \n",
       "12973             40    7.0  \n",
       "12974             40    7.0  \n",
       "12975             40    7.0  \n",
       "\n",
       "[12976 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_min = minimum_scores[X['essay_set']]\n",
    "old_max = maximum_scores[X['essay_set']]\n",
    "old_range = old_max - old_min \n",
    "new_range = (10 - 0)  \n",
    "X['score'] = np.around((((X['domain1_score'] - old_min) * new_range) / old_range) )\n",
    "\n",
    "X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokanization and vectorization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "def wordlist(essay, remove_stopwords):\n",
    "    \n",
    "    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
    "    words = essay.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_sentences(essay, remove_stopwords):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    featureVec = np.zeros((num_features),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of pre processing , tokanization then vectorivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of tokanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 'example', 'sentence', 'explain', 'thw', 'working', 'function'], ['tiwari', 'pakhandi', 'hai', 'shayad', 'isecay', 'samajh', 'aa', 'jayega'], ['hi', 'example', 'sentence', 'explain', 'thw', 'working', 'function', 'tiwari', 'pakhandi', 'hai', 'shayad', 'isecay', 'samajh', 'aa', 'jayega'], ['hi', 'example', 'sentence', 'explain', 'thw', 'working', 'function'], ['tiwari', 'pakhandi', 'hai', 'shayad', 'isecay', 'samajh', 'aa', 'jayega']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "example2='hi I am an example1 sentence here to explain thw working of above function .Tiwari pakhandi hai,shayad isecay samajh me aa jayega'\n",
    "example1='hi I am an example22 sentence here to explain thw working of above function. Tiwari pakhandi hai,shayad isecay samajh me aa jayega'\n",
    "example3='hi I am an example33 sentence here to explain thw working of above function. Tiwari pakhandi hai,shayad isecay samajh me aa jayega'\n",
    "s=[]\n",
    "s +=Make_sentences(example1,remove_stopwords=True)\n",
    "s +=Make_sentences(example2,remove_stopwords=True)\n",
    "s +=Make_sentences(example3,remove_stopwords=True)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=15, vector_size=100, alpha=0.025)\n",
      "['jayega', 'aa', 'samajh', 'isecay', 'shayad', 'hai', 'pakhandi', 'tiwari', 'function', 'working', 'thw', 'explain', 'sentence', 'example', 'hi']\n",
      "[ 7.6937797e-03  9.1159474e-03  1.1371024e-03 -8.3266143e-03\n",
      "  8.4234700e-03 -3.6967925e-03  5.7412465e-03  4.3951930e-03\n",
      "  9.6916147e-03 -9.2963111e-03  9.2080925e-03 -9.2849806e-03\n",
      " -6.9090333e-03 -9.1020577e-03 -5.5503827e-03  7.3705087e-03\n",
      "  9.1668312e-03 -3.3259962e-03  3.7265129e-03 -3.6276821e-03\n",
      "  7.8818426e-03  5.8701234e-03  3.1563587e-07 -3.6258488e-03\n",
      " -7.2205034e-03  4.7711711e-03  1.4503270e-03 -2.6101116e-03\n",
      "  7.8361779e-03 -4.0486371e-03 -9.1493567e-03 -2.2525082e-03\n",
      "  1.2837682e-04 -6.6368566e-03 -5.4893969e-03 -8.5005304e-03\n",
      "  9.2295222e-03  7.4223373e-03 -2.9353739e-04  7.3695402e-03\n",
      "  7.9519944e-03 -7.7969249e-04  6.6087437e-03  3.7650829e-03\n",
      "  5.0741732e-03  7.2507611e-03 -4.7409856e-03 -2.1847258e-03\n",
      "  8.7262300e-04  4.2386064e-03  3.3045979e-03  5.0974917e-03\n",
      "  4.5837699e-03 -8.4385853e-03 -3.1799104e-03 -7.2401213e-03\n",
      "  9.6792430e-03  5.0045145e-03  1.7498615e-04  4.1163173e-03\n",
      " -7.6607405e-03 -6.2946621e-03  3.0802120e-03  6.5347711e-03\n",
      "  3.9474918e-03  6.0163550e-03 -1.9876563e-03 -3.3497475e-03\n",
      "  2.0709650e-04 -3.1957270e-03 -5.5131125e-03 -7.7857119e-03\n",
      "  6.5343911e-03 -1.0947213e-03 -1.8904419e-03 -7.8064846e-03\n",
      "  9.3418444e-03  8.6251087e-04  1.7692363e-03  2.4927426e-03\n",
      " -7.3862029e-03  1.6389518e-03  2.9787791e-03 -8.5606640e-03\n",
      "  4.9517085e-03  2.4325817e-03  7.4991216e-03  5.0448528e-03\n",
      " -3.0327144e-03 -7.1612983e-03  7.0947898e-03  1.9038507e-03\n",
      "  5.2038110e-03  6.3810335e-03  1.9142814e-03 -6.1276453e-03\n",
      " -4.5204279e-06  8.2704099e-03 -6.0930839e-03  9.4411215e-03]\n",
      "Word2Vec(vocab=15, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(s, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.index_to_key )\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model.wv['hi'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onto the real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-efff99078a28>:40: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Traning complete-------------------\n",
      "--------------- Initializing vectorization on train set -------------------\n",
      "--------------completed-----------------\n",
      "--------------- Initializing vectorization on test set -------------------\n",
      "------------Complete----------------------\n",
      "------------Adding extra dimension to essay----------------------\n",
      "------------Complete----------------------\n",
      "------------Traning the Preprocessed Data----------------------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "146/146 [==============================] - 7s 19ms/step - loss: 27.8537 - accuracy: 0.1021 - val_loss: 505.9299 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 7.7979 - accuracy: 0.1206 - val_loss: 459.4788 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "146/146 [==============================] - 2s 10ms/step - loss: 6.1812 - accuracy: 0.1316 - val_loss: 446.8781 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 5.6170 - accuracy: 0.1456 - val_loss: 428.0464 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 4.8814 - accuracy: 0.1428 - val_loss: 439.6848 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 4.8605 - accuracy: 0.1519 - val_loss: 423.1797 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 4.9947 - accuracy: 0.1542 - val_loss: 422.9805 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 4.5379 - accuracy: 0.1446 - val_loss: 405.7713 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 4.3618 - accuracy: 0.1493 - val_loss: 385.1961 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 3.9172 - accuracy: 0.1498 - val_loss: 387.3601 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 4.1931 - accuracy: 0.1489 - val_loss: 403.0940 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.8403 - accuracy: 0.1557 - val_loss: 387.8442 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.8480 - accuracy: 0.1460 - val_loss: 376.4778 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 3.7692 - accuracy: 0.1512 - val_loss: 383.0247 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.4865 - accuracy: 0.1535 - val_loss: 364.3218 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 3.3403 - accuracy: 0.1536 - val_loss: 366.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 3.4730 - accuracy: 0.1524 - val_loss: 342.8807 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 3.4321 - accuracy: 0.1496 - val_loss: 348.6716 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 3.2847 - accuracy: 0.1530 - val_loss: 359.9022 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.4474 - accuracy: 0.1529 - val_loss: 340.8560 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 3.0305 - accuracy: 0.1515 - val_loss: 355.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.2693 - accuracy: 0.1499 - val_loss: 340.2307 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 3.0123 - accuracy: 0.1506 - val_loss: 347.8596 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 3.1866 - accuracy: 0.1547 - val_loss: 367.6962 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "146/146 [==============================] - 2s 13ms/step - loss: 3.1942 - accuracy: 0.1545 - val_loss: 350.9894 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "146/146 [==============================] - 2s 14ms/step - loss: 3.0368 - accuracy: 0.1427 - val_loss: 325.8006 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 2.8860 - accuracy: 0.1481 - val_loss: 337.1550 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 2.7993 - accuracy: 0.1538 - val_loss: 311.2565 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 2.9373 - accuracy: 0.1563 - val_loss: 329.4085 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 2.7190 - accuracy: 0.1463 - val_loss: 341.3502 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "146/146 [==============================] - 2s 11ms/step - loss: 2.8632 - accuracy: 0.1486 - val_loss: 312.7503 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "146/146 [==============================] - 1s 10ms/step - loss: 2.9077 - accuracy: 0.1432 - val_loss: 316.3276 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 2.8429 - accuracy: 0.1540 - val_loss: 298.7555 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 2.7309 - accuracy: 0.1427 - val_loss: 313.3689 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 2.6906 - accuracy: 0.1511 - val_loss: 337.4951 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 2.5591 - accuracy: 0.1449 - val_loss: 312.5128 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      " 25/146 [====>.........................] - ETA: 1s - loss: 2.3778 - accuracy: 0.1453"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "skf = KFold(n_splits=5,shuffle=True)\n",
    "count=1\n",
    "results=[]\n",
    "y_pred_list = []\n",
    "for train,test in skf.split(X,Y):\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "        X_test, X_train, y_test, y_train = X.iloc[test], X.iloc[train], Y.iloc[test], Y.iloc[train]\n",
    "        trainE = X_train['essay']\n",
    "        testE= X_test['essay']\n",
    "        sentences=[]\n",
    "        for essay in trainE:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        \n",
    "            sentences +=Make_sentences(essay, remove_stopwords = True)\n",
    "        \n",
    "        #Initializing different parameters for the word2vec model to be used\n",
    "    \n",
    "        num_features = 300\n",
    "        min_word_count = 40\n",
    "        num_workers = 4\n",
    "        context = 10\n",
    "        downsampling = 1e-3\n",
    "\n",
    "        print(\"Training Word2Vec Model...\")\n",
    "        \n",
    "        #Initializing model fro vectorization\n",
    "        \n",
    "    \n",
    "        #initializing model and loading parameters\n",
    "    \n",
    "        model = Word2Vec(sentences,vector_size=300, workers=num_workers, min_count = min_word_count, window = context)\n",
    "        #avoiding normalization to not reduce the essence of some words used in context\n",
    "    \n",
    "        model.init_sims(replace=True)\n",
    "    \n",
    "        #saving model\n",
    "    \n",
    "        model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "        \n",
    "        print(\"--------------- Traning complete-------------------\")\n",
    "        \n",
    "        print(\"--------------- Initializing vectorization on train set -------------------\")\n",
    "        \n",
    "        \n",
    "        #generating vectors train\n",
    "    \n",
    "        clean_train = []\n",
    "        for essay in trainE:\n",
    "            clean_train.append(wordlist(essay, remove_stopwords=True))\n",
    "        trainDataVecs = getAvgFeatureVecs(clean_train, model, num_features)\n",
    "    \n",
    "        print(\"--------------completed-----------------\")\n",
    "    \n",
    "        print(\"--------------- Initializing vectorization on test set -------------------\")\n",
    "        \n",
    "        #generating vectors for test\n",
    "    \n",
    "        clean_test = []\n",
    "        for essay in testE:\n",
    "            clean_test.append(wordlist(essay, remove_stopwords=True))\n",
    "        testDataVecs = getAvgFeatureVecs(clean_test, model, num_features)\n",
    "        \n",
    "        trainDataVecs = np.array(trainDataVecs)\n",
    "        testDataVecs = np.array(testDataVecs)\n",
    "        \n",
    "        print(\"------------Complete----------------------\") \n",
    "        \n",
    "        print(\"------------Adding extra dimension to essay----------------------\") \n",
    "        \n",
    "        #Adding extra dimension to essay\n",
    "        \n",
    "        trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "        \n",
    "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "        \n",
    "        print(\"------------Complete----------------------\") \n",
    "        \n",
    "        print(\"------------Traning the Preprocessed Data----------------------\") \n",
    "        \n",
    "        #Traning the Preprocessed Data\n",
    "        \n",
    "        lstm_model = get_model()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "        \n",
    "       \n",
    "        \n",
    "        history=lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50,validation_split=0.1)\n",
    "    # Save any one of the 5 models.\n",
    "        if count == 5:\n",
    "             lstm_model.save('./final_lstm.h5')\n",
    "    \n",
    "        \n",
    "        print('-----------Complete--------------')\n",
    "        \n",
    "        \n",
    "        print('---------------Predicting test set------------------------')\n",
    "         \n",
    "        #predicting test\n",
    "        \n",
    "        y_pred = lstm_model.predict(testDataVecs)\n",
    "        \n",
    "        # Save any one of the 8 models.\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Round y_pred to the nearest integer.\n",
    "        y_pred = np.around(y_pred)\n",
    "        y_pred_list.append(y_pred)\n",
    "            # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "        result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "        print(\"Kappa Score: {}\".format(result))\n",
    "        results.append(result)\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'],'r',label='accuracy')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.load_weights(\"./final_lstm.h5\")\n",
    "clean_test_essays = []\n",
    "clean_test_essays.append(wordlist( a, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "preds = lstm_model.predict(testDataVecs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
