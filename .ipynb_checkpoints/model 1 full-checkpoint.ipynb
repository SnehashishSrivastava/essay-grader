{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR='Dataset and discription/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(columns=['rater1_domain1','rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score  \n",
       "0                  8  \n",
       "1                  9  \n",
       "2                  7  \n",
       "3                 10  \n",
       "4                  8  \n",
       "...              ...  \n",
       "12971             35  \n",
       "12972             32  \n",
       "12973             40  \n",
       "12974             40  \n",
       "12975             40  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=X['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n",
    "maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score     score  \n",
       "0                  8  6.000000  \n",
       "1                  9  7.000000  \n",
       "2                  7  5.000000  \n",
       "3                 10  8.000000  \n",
       "4                  8  6.000000  \n",
       "...              ...       ...  \n",
       "12971             35  5.833333  \n",
       "12972             32  5.333333  \n",
       "12973             40  6.666667  \n",
       "12974             40  6.666667  \n",
       "12975             40  6.666667  \n",
       "\n",
       "[12976 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_min = minimum_scores[X['essay_set']]\n",
    "old_max = maximum_scores[X['essay_set']]\n",
    "old_range = old_max - old_min \n",
    "new_range = (10 - 0)  \n",
    "X['score'] = (((X['domain1_score'] - old_min) * new_range) / old_range) \n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "def wordlist(essay, remove_stopwords):\n",
    "    \n",
    "    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
    "    words = essay.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_sentences(essay, remove_stopwords):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    featureVec = np.zeros((num_features),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.3, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\sneha\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.21 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\sneha\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-7320736e9f54>:38: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "123/123 [==============================] - 6s 21ms/step - loss: 103.8878 - accuracy: 0.0793 - val_loss: 38.9480 - val_accuracy: 0.1115\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 38.3345 - accuracy: 0.0900 - val_loss: 27.8191 - val_accuracy: 0.0851\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 31.7840 - accuracy: 0.0719 - val_loss: 22.1192 - val_accuracy: 0.0782\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 26.9797 - accuracy: 0.0684 - val_loss: 18.3416 - val_accuracy: 0.0816\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 22.5356 - accuracy: 0.0714 - val_loss: 15.5357 - val_accuracy: 0.1011\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 21.6387 - accuracy: 0.0897 - val_loss: 13.8200 - val_accuracy: 0.1322\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 17.4420 - accuracy: 0.1177 - val_loss: 11.8697 - val_accuracy: 0.1402\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 17.0223 - accuracy: 0.1254 - val_loss: 9.9853 - val_accuracy: 0.1425\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 13.0515 - accuracy: 0.1278 - val_loss: 8.2921 - val_accuracy: 0.1425\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 11.5408 - accuracy: 0.1371 - val_loss: 7.2386 - val_accuracy: 0.1425\n",
      "Epoch 11/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 10.1814 - accuracy: 0.1268 - val_loss: 6.6621 - val_accuracy: 0.1425\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 8.3588 - accuracy: 0.1322 - val_loss: 5.9286 - val_accuracy: 0.1425\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 9.1748 - accuracy: 0.1329 - val_loss: 5.8498 - val_accuracy: 0.1425\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 8.6357 - accuracy: 0.1299 - val_loss: 5.7378 - val_accuracy: 0.1425\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 8.6698 - accuracy: 0.1363 - val_loss: 5.6767 - val_accuracy: 0.1425\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 8.1119 - accuracy: 0.1279 - val_loss: 5.4119 - val_accuracy: 0.1425\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.6111 - accuracy: 0.1270 - val_loss: 5.3845 - val_accuracy: 0.1425\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.9625 - accuracy: 0.1362 - val_loss: 5.2757 - val_accuracy: 0.1425\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.6963 - accuracy: 0.1296 - val_loss: 5.2198 - val_accuracy: 0.1425\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.3184 - accuracy: 0.1297 - val_loss: 5.3725 - val_accuracy: 0.1425\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 6.6965 - accuracy: 0.1306 - val_loss: 5.1701 - val_accuracy: 0.1425\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.0243 - accuracy: 0.1382 - val_loss: 5.1000 - val_accuracy: 0.1425\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.6486 - accuracy: 0.1363 - val_loss: 5.1167 - val_accuracy: 0.1425\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.7336 - accuracy: 0.1236 - val_loss: 5.0756 - val_accuracy: 0.1425\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.7700 - accuracy: 0.1329 - val_loss: 5.2143 - val_accuracy: 0.1425\n",
      "Epoch 26/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.1409 - accuracy: 0.1303 - val_loss: 5.1064 - val_accuracy: 0.1425\n",
      "Epoch 27/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 7.2322 - accuracy: 0.1302 - val_loss: 5.0329 - val_accuracy: 0.1425\n",
      "Epoch 28/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.1780 - accuracy: 0.1353 - val_loss: 5.1774 - val_accuracy: 0.1425\n",
      "Epoch 29/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.4995 - accuracy: 0.1262 - val_loss: 5.0064 - val_accuracy: 0.1425\n",
      "Epoch 30/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.8497 - accuracy: 0.1258 - val_loss: 5.0536 - val_accuracy: 0.1425\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.9157 - accuracy: 0.1279 - val_loss: 4.9587 - val_accuracy: 0.1425\n",
      "Epoch 32/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.8566 - accuracy: 0.1272 - val_loss: 5.2474 - val_accuracy: 0.1425\n",
      "Epoch 33/50\n",
      "123/123 [==============================] - 1s 11ms/step - loss: 6.2098 - accuracy: 0.1283 - val_loss: 4.8783 - val_accuracy: 0.1425\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 1s 11ms/step - loss: 6.7447 - accuracy: 0.1214 - val_loss: 4.9638 - val_accuracy: 0.1425\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.1350 - accuracy: 0.1326 - val_loss: 4.9830 - val_accuracy: 0.1425\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.0468 - accuracy: 0.1326 - val_loss: 4.7830 - val_accuracy: 0.1425\n",
      "Epoch 37/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.2501 - accuracy: 0.1330 - val_loss: 4.7833 - val_accuracy: 0.1425\n",
      "Epoch 38/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.0742 - accuracy: 0.1273 - val_loss: 4.8894 - val_accuracy: 0.1425\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.9297 - accuracy: 0.1347 - val_loss: 4.8778 - val_accuracy: 0.1425\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.7063 - accuracy: 0.1267 - val_loss: 4.8463 - val_accuracy: 0.1425\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.2252 - accuracy: 0.1252 - val_loss: 4.8231 - val_accuracy: 0.1425\n",
      "Epoch 42/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 6.5139 - accuracy: 0.1308 - val_loss: 4.7265 - val_accuracy: 0.1425\n",
      "Epoch 43/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.3848 - accuracy: 0.1292 - val_loss: 4.7847 - val_accuracy: 0.1425\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.7542 - accuracy: 0.1307 - val_loss: 4.6615 - val_accuracy: 0.1414\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.4811 - accuracy: 0.1299 - val_loss: 4.7782 - val_accuracy: 0.1402\n",
      "Epoch 46/50\n",
      "123/123 [==============================] - 1s 11ms/step - loss: 5.7624 - accuracy: 0.1340 - val_loss: 4.7165 - val_accuracy: 0.1402\n",
      "Epoch 47/50\n",
      "123/123 [==============================] - 1s 11ms/step - loss: 5.4151 - accuracy: 0.1299 - val_loss: 4.8523 - val_accuracy: 0.1402\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.9785 - accuracy: 0.1296 - val_loss: 4.7251 - val_accuracy: 0.1402\n",
      "Epoch 49/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.6597 - accuracy: 0.1348 - val_loss: 4.8457 - val_accuracy: 0.1402\n",
      "Epoch 50/50\n",
      "123/123 [==============================] - 1s 10ms/step - loss: 5.5068 - accuracy: 0.1272 - val_loss: 4.5529 - val_accuracy: 0.1379\n",
      "Kappa Score: 0.9583194188078845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "results=[]\n",
    "y_pred_list = []\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X,Y, test_size=0.33, shuffle= True)\n",
    "\n",
    "trainE = X_train['essay']\n",
    "testE= X_test['essay']\n",
    "sentences=[]\n",
    "for essay in trainE:\n",
    "        # Obtaining all sentences from the training essays.    \n",
    "    sentences +=Make_sentences(essay, remove_stopwords = True)\n",
    "        \n",
    "        #Initializing different parameters for the word2vec model to be used\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec Model...\")\n",
    "        \n",
    "#Initializing model fro vectorization\n",
    "        \n",
    "    \n",
    "        #initializing model and loading parameters\n",
    "    \n",
    "model = Word2Vec(sentences,vector_size=300, workers=num_workers, min_count = min_word_count, window = context)\n",
    "        #avoiding normalization to not reduce the essence of some words used in context\n",
    "    \n",
    "model.init_sims(replace=True)\n",
    "    \n",
    "        #saving model\n",
    "    \n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "clean_train = []\n",
    "\n",
    "for essay in trainE:\n",
    "    clean_train.append(wordlist(essay, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train, model, num_features)  \n",
    "\n",
    "clean_test = []\n",
    "for essay in testE:\n",
    "    clean_test.append(wordlist(essay, remove_stopwords=True))\n",
    "                                      \n",
    "testDataVecs = getAvgFeatureVecs(clean_test, model, num_features)\n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))        \n",
    "    \n",
    "lstm_model = get_model()\n",
    "        \n",
    "        \n",
    "        \n",
    "history=lstm_model.fit(trainDataVecs, Y_train, batch_size=64, epochs=50,validation_split=0.1)\n",
    "\n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "y_pred = lstm_model.predict(testDataVecs)\n",
    "        \n",
    "    \n",
    "y_pred = np.around(y_pred)\n",
    "y_pred_list.append(y_pred)\n",
    "lstm_model.save('./finalnew_lstm.h5')           \n",
    "result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))\n",
    "results.append(result)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=r\"Dear local newspaper, @CAPS1 the caveman found fire, ever, thought what would happen if he did not use it? Well that is where we are today. The computer is our fire. Some people might not be able to compare a computer to fire, but the advancement in technology is just about the same. There are now tons of people who use computer. I definetly agree that we should continue the use of computers. Computers help people learn, we can communicate better, and it will allow us to advance farther. The computer can help us learn. It can teach us about faraway places that we probably would never have known that they existed. We can learn about the weather better. If we did not did not have computers we would have never left earth. Or be able to make @CAPS2. The computer has already helped us a math teacher, @PERSON1 said Computers have boosted the kids knowledge @CAPS5 much. I never learned some of the stuff @CAPS1 i was growing up. This next generation will be the we have had yet. @CAPS3 more than we could have alone, @CAPS5 why get rid of it? The computer can also help us communicate better and faster. which the avoke in the @LOCATION2 happened, it might have taken a weck or even a month for the @LOCATION1 to hear about it, and longer to try and go down to help. By then the people would have to try and fix it. it up themselves for two months or more. I remember yesterday. I had a board of education meeting to go to. It was really importan checking my @CAPS4-mails on the computer and I found out that i needed to go. It was better that i found out and not the next day @CAPS1 my boss was going to yell at me because i did not go. That is how useful the computer is to communicate. The computer is also goind to help us @CAPS3 farther. It is like the fire. It helped People @CAPS3 @CAPS5 much, @CAPS5 once the computer is at its peak, it will push us just as for the fire did. Survey shows that @PERCENT1 of the children population is already smarter than the adult population. @CAPS5 @CAPS1 the children become adults there is nothing this world cannot do. we would be able to colonize the galaxy! @CAPS5, as you can see, computers are more beneficial than bad. they help teach people new stuff, communicate faster and better, and will help us @CAPS3 farther. Think about it, millions of years from now, people will say, @CAPS6 if man never made computers, that would be worse than the fall of empire. @CAPS7 you for taking the time to read this letter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.load_weights(\"./final_lstm.h5\")\n",
    "clean_test_essays = []\n",
    "clean_test_essays.append(wordlist( a, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "preds = lstm_model.predict(testDataVecs)\n",
    "print(np.around(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
